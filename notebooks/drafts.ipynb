{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "import json, pickle"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test 1: Total Number of Rows Check "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# 1. no. of rows check before processing\r\n",
    "\r\n",
    "raw_files = ['02-21.10way.clean.txt','22.auto.clean.txt','23.auto.clean.txt']\r\n",
    "raw_filepath = 'C:/Users/rahin/projects/syntax-tree-rnn/data/raw/'\r\n",
    "\r\n",
    "for file in raw_files:\r\n",
    "    with open(raw_filepath+file,'r') as f:\r\n",
    "        contents = f.readlines()\r\n",
    "    total_lines = 0\r\n",
    "    for line in contents:\r\n",
    "        total_lines +=1\r\n",
    "    print(f\"This raw dataset <{file}> has {total_lines} lines\")\r\n",
    "\r\n",
    "# 2. no. of json objects after processing\r\n",
    "processed_files = ['train_dataset_transformed.json','valid_dataset_transformed.json','test_dataset_transformed.json']\r\n",
    "processed_filepath = 'C:/Users/rahin/projects/syntax-tree-rnn/data/interim/'\r\n",
    "\r\n",
    "for file in processed_files:\r\n",
    "    with open(processed_filepath+file,) as json_file:\r\n",
    "        pp_data = json.load(json_file)\r\n",
    "\r\n",
    "    print(f\"This processed dataset <{file}> has {len(pp_data)} lines\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "This raw dataset <02-21.10way.clean.txt> has 39832 lines\n",
      "This raw dataset <22.auto.clean.txt> has 1700 lines\n",
      "This raw dataset <23.auto.clean.txt> has 2416 lines\n",
      "This processed dataset <train_dataset_transformed.json> has 39832 lines\n",
      "This processed dataset <valid_dataset_transformed.json> has 1700 lines\n",
      "This processed dataset <test_dataset_transformed.json> has 2416 lines\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Test 2: No missing entities after Syntax Tree Transformation Check:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "# let's take 3 examples from raw file:\r\n",
    "raw_filepath = 'C:/Users/rahin/projects/syntax-tree-rnn/data/raw/'\r\n",
    "with open(raw_filepath+'23.auto.clean.txt','r') as f:\r\n",
    "    contents = f.readlines()\r\n",
    "examples = []\r\n",
    "examples.append(contents[8])\r\n",
    "examples.append(contents[29])\r\n",
    "examples.append(contents[32])\r\n",
    "print(examples)\r\n",
    "\r\n",
    "# same 3 examples in json format:\r\n",
    "interim_filepath = 'C:/Users/rahin/projects/syntax-tree-rnn/data/interim/'\r\n",
    "with open(interim_filepath+'test_dataset_transformed.json') as f:\r\n",
    "    json_contents = json.load(f)\r\n",
    "transformed_examples = []\r\n",
    "transformed_examples.append(json_contents[8])\r\n",
    "transformed_examples.append(json_contents[29])\r\n",
    "transformed_examples.append(json_contents[32])"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "['(TOP (S (`` ``) (NP (DT The) (NN equity) (NN market)) (VP (VBD was) (ADJP (JJ illiquid))) (. .)))\\n', '(TOP (S (NP (DT The) (NN market)) (VP (VBD crumbled)) (. .)))\\n', '(TOP (S (NP (DT These) (NNS stocks)) (ADVP (RB eventually)) (VP (VBD reopened)) (. .)))\\n']\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "import nltk\r\n",
    "from nltk.tree import Tree\r\n",
    "parse_tree = Tree.fromstring(examples[0])\r\n",
    "print(parse_tree)\r\n",
    "# examples\r\n",
    "# parse_tree"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(TOP\n",
      "  (S\n",
      "    (`` ``)\n",
      "    (NP (DT The) (NN equity) (NN market))\n",
      "    (VP (VBD was) (ADJP (JJ illiquid)))\n",
      "    (. .)))\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "transformed_examples[0]"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'2': {'tokens': [['``'],\n",
       "   ['The'],\n",
       "   ['equity'],\n",
       "   ['market'],\n",
       "   ['was'],\n",
       "   ['illiquid'],\n",
       "   ['.']],\n",
       "  'tags': ['``', 'DT', 'NN', 'NN', 'VBD', 'JJ', '.'],\n",
       "  'targets': ['O', 'B-NP', 'I-NP', 'E-NP', 'O', 'ADJP', 'O']},\n",
       " '3': {'tokens': [['``'],\n",
       "   ['The', 'equity', 'market'],\n",
       "   ['was'],\n",
       "   ['illiquid'],\n",
       "   ['.']],\n",
       "  'tags': ['O', 'NP', 'O', 'ADJP', 'O'],\n",
       "  'targets': ['O', 'O', 'B-VP', 'E-VP', 'O']},\n",
       " '4': {'tokens': [['``'],\n",
       "   ['The', 'equity', 'market'],\n",
       "   ['was', 'illiquid'],\n",
       "   ['.']],\n",
       "  'tags': ['O', 'O', 'VP', 'O'],\n",
       "  'targets': ['B-S', 'I-S', 'I-S', 'E-S']},\n",
       " '5': {'tokens': [['``', 'The', 'equity', 'market', 'was', 'illiquid', '.']],\n",
       "  'tags': ['S'],\n",
       "  'targets': []}}"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## View: Look-up tables for further analysis:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "source": [
    "def load_pickle(input_filename):\r\n",
    "    filepath = 'C:/Users/rahin/projects/syntax-tree-rnn/data/interim/'\r\n",
    "    with open(filepath+input_filename, 'rb') as f:\r\n",
    "        return pickle.load(f)\r\n",
    "pkl_lst = ['tokens_lkp.pkl','tags_lkp.pkl','targets_lkp.pkl']\r\n",
    "\r\n",
    "tokens_lkp = load_pickle(pkl_lst[0])\r\n",
    "tags_lkp = load_pickle(pkl_lst[1])\r\n",
    "targets_lkp = load_pickle(pkl_lst[2])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "source": [
    "print(f\"Tokens look-up dict:\\n{list(tokens_lkp.items())[:10]}\")\r\n",
    "print(f\"POS Tags look-up dict:\\n{list(tags_lkp.items())[:10]}\")\r\n",
    "print(f\"Targets look-up dict:\\n{list(targets_lkp.items())[:10]}\")"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Tokens look-up dict:\n",
      "[('viciously', 0), ('176,470', 1), ('woven', 2), ('bodes', 3), ('Investigation', 4), ('39.68', 5), ('tacked-down', 6), ('lounges', 7), ('coverings', 8), ('8.30', 9)]\n",
      "POS Tags look-up dict:\n",
      "[('SYM', 0), ('CC', 1), ('NNPS', 2), ('$', 3), ('JJR', 4), ('PRP', 5), ('WP$', 6), ('-LRB-', 7), ('VBP', 8), ('#', 9)]\n",
      "Targets look-up dict:\n",
      "[('I-ADVP', 0), ('E-NAC', 1), ('B-FRAG', 2), ('E-CONJP', 3), ('E-ADJP', 4), ('B-PRN', 5), ('WHADVP', 6), ('NAC', 7), ('E-X', 8), ('I-UCP', 9)]\n"
     ]
    }
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.8",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('tf': conda)"
  },
  "interpreter": {
   "hash": "75c4db28bb58e6de10e05be21b6046b5ba21d9aba4af4007d97c2f3325bc0896"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}